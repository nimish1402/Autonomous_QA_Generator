# Environment Configuration for Autonomous QA Agent
# Copy this file to .env and set your API keys

# ========================================
# LLM API Configuration (Choose One)
# ========================================

# Option 1: Google Gemini (Recommended - free tier available)
# GEMINI_API_KEY=your_gemini_api_key_here
# GEMINI_MODEL=gemini-1.5-flash
# GEMINI_MAX_TOKENS=2000
# GEMINI_TEMPERATURE=0.1

# Option 2: OpenAI GPT
# OPENAI_API_KEY=your_openai_api_key_here
# OPENAI_MODEL=gpt-3.5-turbo
# OPENAI_MAX_TOKENS=2000
# OPENAI_TEMPERATURE=0.1

# Option 3: Anthropic Claude
# ANTHROPIC_API_KEY=your_anthropic_api_key_here  
# ANTHROPIC_MODEL=claude-3-sonnet-20240229
# ANTHROPIC_MAX_TOKENS=2000
# ANTHROPIC_TEMPERATURE=0.1

# Option 4: Local Ollama (Free, runs locally)
# Install Ollama: https://ollama.ai/
# Then: ollama pull llama2
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=llama2
# OLLAMA_TEMPERATURE=0.1

# Option 5: HuggingFace Inference API
# HUGGINGFACE_TOKEN=your_hf_token_here
# HF_MODEL=microsoft/DialoGPT-medium
# HF_MAX_TOKENS=1000

# ========================================
# Application Configuration
# ========================================

# Backend settings
# BACKEND_HOST=0.0.0.0
# BACKEND_PORT=8000

# Frontend settings  
# FRONTEND_PORT=8501

# Vector database settings
# VECTOR_DB_PATH=./vectordb
# EMBEDDING_MODEL=all-MiniLM-L6-v2

# ========================================
# How to Use:
# ========================================
# 1. Copy this file: cp .env.example .env
# 2. Uncomment and set ONE of the LLM provider options above
# 3. Restart the backend: python run.py
# 
# Note: Without API keys, the system will use template-based
# generation which works but provides less sophisticated results.